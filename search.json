[{"title":"浅谈深度学习中的激活函数 The Activation Function in Deep Learning","url":"/2016/11/11/activation-function-in-dl/","content":"激活函数的作用首先，激活函数不是真的要去激活什么。在神经网络中，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。\n\n比如在下面的这个问题中：\n如上图(图片来源)，在最简单的情况下，数据是线性可分的，只需要一条直线就已经能够对样本进行很好地分类。但如果情况变得复杂了一点呢？在上图中(图片来源)，数据就变成了线性不可分的情况。在这种情况下，简单的一条直线就已经不能够对样本进行很好地分类了。于是我们尝试引入非线性的因素，对样本进行分类。\n在神经网络中也类似，我们需要引入一些非线性的因素，来更好地解决复杂的问题。而激活函数恰好能够帮助我们引入非线性因素，它使得我们的神经网络能够更好地解决较为复杂的问题。\n\n激活函数的定义及其相关概念在ICML2016的一篇论文Noisy Activation Functions中，作者将激活函数定义为一个几乎处处可微的 h : R → R 。\n在实际应用中，我们还会涉及到以下的一些概念：a.饱和当一个激活函数$h(x)$满足$$\\lim_{n\\to +\\infty} h’(x)&#x3D;0$$时我们称之为右饱和。\n当一个激活函数$h(x)$满足$$\\lim_{n\\to -\\infty} h’(x)&#x3D;0$$时我们称之为左饱和。当一个激活函数，既满足左饱和又满足又饱和时，我们称之为饱和。\nb.硬饱和与软饱和对任意的$x$，如果存在常数$c$，当$x &gt; c$时恒有 $h’(x) &#x3D; 0$则称其为右硬饱和，当$x &lt; c$时恒 有$h’(x)&#x3D;0$则称其为左硬饱和。若既满足左硬饱和，又满足右硬饱和，则称这种激活函数为硬饱和。但如果只有在极限状态下偏导数等于0的函数，称之为软饱和。\n\nSigmoid函数Sigmoid函数曾被广泛地应用，但由于其自身的一些缺陷，现在很少被使用了。Sigmoid函数被定义为：$$f(x)&#x3D;\\frac{1}{1+e^{-x}}$$函数对应的图像是：\n优点：\n\nSigmoid函数的输出映射在$(0,1)$之间，单调连续，输出范围有限，优化稳定，可以用作输出层。\n求导容易。\n\n缺点：\n\n由于其软饱和性，容易产生梯度消失，导致训练出现问题。\n其输出并不是以0为中心的。\n\n\ntanh函数现在，比起Sigmoid函数我们通常更倾向于tanh函数。tanh函数被定义为$$tanh(x)&#x3D;\\frac{1-e^{-2x}}{1+e^{-2x}}$$函数位于[-1, 1]区间上，对应的图像是：优点：\n\n比Sigmoid函数收敛速度更快。\n相比Sigmoid函数，其输出以0为中心。缺点：还是没有改变Sigmoid函数的最大问题——由于饱和性产生的梯度消失。\n\n\nReLUReLU是最近几年非常受欢迎的激活函数。被定义为$$y&#x3D;\\begin{cases}0&amp; (x\\le0)\\\\x&amp; (x&gt;0)\\end{cases}$$对应的图像是：但是除了ReLU本身的之外，TensorFlow还提供了一些相关的函数，比如定义为min(max(features, 0), 6)的tf.nn.relu6(features, name=None)或是CReLU，即tf.nn.crelu(features, name=None)。其中CReLU部分可以参考这篇论文。优点：\n\n相比起Sigmoid和tanh，ReLU(e.g. a factor of 6 in Krizhevsky et al.)在SGD中能够快速收敛。例如在下图的实验中，在一个四层的卷积神经网络中，实线代表了ReLU，虚线代表了tanh，ReLU比起tanh更快地到达了错误率0.25处。据称，这是因为它线性、非饱和的形式。\nSigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现。\n有效缓解了梯度消失的问题。\n在没有无监督预训练的时候也能有较好的表现。\n提供了神经网络的稀疏表达能力。\n\n缺点：随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了。\n\nLReLU、PReLU与RReLU\n通常在LReLU和PReLU中，我们定义一个激活函数为$$f(y_i)&#x3D;\\begin{cases}y_i&amp; if(y_i&gt;0)\\\\a_iy_i&amp; if(y_i\\le0)\\end{cases}$$\n\nLReLU当$a_i$比较小而且固定的时候，我们称之为LReLU。LReLU最初的目的是为了避免梯度消失。但在一些实验中，我们发现LReLU对准确率并没有太大的影响。很多时候，当我们想要应用LReLU时，我们必须要非常小心谨慎地重复训练，选取出合适的$a$，LReLU的表现出的结果才比ReLU好。因此有人提出了一种自适应地从数据中学习参数的PReLU。\n\nPReLUPReLU是LReLU的改进，可以自适应地从数据中学习参数。PReLU具有收敛速度快、错误率低的特点。PReLU可以用于反向传播的训练，可以与其他层同时优化。 在论文Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification中，作者就对比了PReLU和ReLU在ImageNet model A的训练效果。值得一提的是，在tflearn中有现成的LReLU和PReLU可以直接用。\n\nRReLU在RReLU中，我们有$$y_{ji}&#x3D;\\begin{cases}x_{ji}&amp; if(x_{ji}&gt;0)\\\\a_{ji}x_{ji}&amp; if(x_{ji}\\le0)\\end{cases}$$$$a_{ji} \\sim U(l,u),l&lt;u\\ and \\ l,u\\in [0,1) $$其中，$a_{ji}$是一个保持在给定范围内取样的随机变量，在测试中是固定的。RReLU在一定程度上能起到正则效果。在论文Empirical Evaluation of Rectified Activations in Convolution Network中，作者对比了RReLU、LReLU、PReLU、ReLU 在CIFAR-10、CIFAR-100、NDSB网络中的效果。\n\n\n\nELUELU被定义为$$f(x)&#x3D;\\begin{cases}a(e^x-1)&amp; if(x&lt;0)\\\\x&amp; if(0\\le x)\\end{cases}$$其中$a&gt;0$。\n优点：\n\nELU减少了正常梯度与单位自然梯度之间的差距，从而加快了学习。\n在负的限制条件下能够更有鲁棒性。\n\nELU相关部分可以参考这篇论文。\n\nSoftplus与SoftsignSoftplus被定义为$$f(x)&#x3D;log(e^x+1)$$Softsign被定义为$$f(x)&#x3D;\\frac{x}{|x|+1}$$目前使用的比较少，在这里就不详细讨论了。TensorFlow里也有现成的可供使用。激活函数相关TensorFlow的官方文档\n\n总结关于激活函数的选取，目前还不存在定论，实践过程中更多还是需要结合实际情况，考虑不同激活函数的优缺点综合使用。同时，也期待越来越多的新想法，改进目前存在的不足。\n\n文章部分图片或内容参考自：CS231n Convolutional Neural Networks for Visual RecognitionQuora - What is the role of the activation function in a neural network?深度学习中的激活函数导引Noisy Activation Functions－ICML2016\n\n\n本文为作者的个人学习笔记，转载请先声明。如有疏漏，欢迎指出，不胜感谢。\n","categories":["Machine Learning","Deep Learning"],"tags":["Machine Learning","Deep Learning"]},{"title":"深度学习笔记 (一)  卷积神经网络基础 (Foundation of Convolutional Neural Networks)","url":"/2016/11/07/foundation-of-cnn-1/","content":"卷积卷积神经网络(Convolutional Neural Networks)是一种在空间上共享参数的神经网络。使用数层卷积，而不是数层的矩阵相乘。在图像的处理过程中，每一张图片都可以看成一张“薄饼”，其中包括了图片的高度、宽度和深度（即颜色，用RGB表示）。\n\n\n在不改变权重的情况下，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度不同，而且深度也不同的新图像。\n卷积时有很多种填充图像的方法，以下主要介绍两种，一种是相同填充，一种是有效填充。\n如图中紫色方框所示，左边是有效填充，右边是相同填充。在相同填充中，超出边界的部分使用补充0的办法，使得输入输出的图像尺寸相同。而在有效填充中，则不使用补充0的方法，不能超出边界，因此往往输入的尺寸大于输出的尺寸。\n下图展示了以3x3的网格在28x28的图像上，使用不同步长、填充方法填充所得到的输出图像的尺寸：\n下面借助两个动图来理解一下卷积的过程：\n第一种是以3x3的网格在5x5的图像上进行有效填充的卷积过程：\n第二种是使用3x3的网格在5x5图像上进行相同填充的卷积过程，动图在：http://cs231n.github.io/convolutional-networks/\n回顾整个过程，就是一层一层地增加网络深度，最终得到一个又深又窄的表示，然后把其连接到全连接层，然后训练分类器。\n\n局部连接与权重共享\n总体而言，局部连接和权重共享都是减少参数的办法，使得特征提取更为有效。\n上图中左半部分，是全连接神经网络的示例。图中是一个1000x1000的图像，下一隐藏层有$10^6$个神经元，那么就会有1000x1000x$10^6$&#x3D;$10^{12}$个参数。\n上图右半部分，是局部连接神经网络的示例。图中依然是一个1000x1000的图像，下一隐藏层有$10^6$个神经元，但是使用了一个10x10的卷积核，连接到了10x10的局部图像，那么则会有10x10x$10^6$&#x3D;$10^8$个参数。\n可见局部连接可以很大幅度减少参数的数量。\n\n在实际应用中，有一些情况比较特殊，涉及到了统计不变性的问题。比如我们想识别图像中的动物类别，那么动物在图片中的位置（左上角、中间或是右下角）是不重要的，这叫平移不变性；再比如说，在识别数字的过程中，数字的颜色并不影响结果；又或者说，在语言处理中，一些词汇在句子中的位置并不影响其代表的含义。当两种输入可以获得同样的信息，那么我们就应该共享权重而且利用这些输入来共同训练权重。\n在上图中的左半部分，是未使用权重共享的局部连接神经网络的示例。\n在上图中的右半部分，则使用了权重共享。图中是一个1000x1000的图像，有100个10x10的卷积核，最终会有100x10x10&#x3D;10k个参数。使用局部连接和权重共享都大大地减小了参数数量。而共享权重使得统计不变性问题得到了有效解决。\n\n池化\n通过卷积后，为了引入不变性，同时防止过拟合问题或欠拟合问题、降低计算量，我们常进行池化处理。池化过程如上图所示。因此池化过后，通常图像的宽度和高度会变成原来的1&#x2F;2。\n其中包括了Max pooling 、 Mean pooling和Stochastic pooling三种池化方法。\n两种较为常用的是：Max pooling和Mean pooling。Max pooling是选择kernel范围之内的最大值；Mean pooling则是选择kernel范围之内的平均值。\n\nLeNet-5中的卷积与池化分析\n在LeNet-5中，输入层是32x32的尺寸。\n在第一次卷积中，使用了6个卷积核，得到了C1:6张28x28的特征图。\n然后进行下采样，得到S2:特征图宽、高变为原来的1&#x2F;2，即28&#x2F;2&#x3D;14，特征图尺寸变为14x14，特征图张数不变。\n再进行第二次卷积，使用了16个卷积核，得到了C3:16张10x10的特征图。\n然后进行下采样，得到S4:特征图宽、高变为原来的1&#x2F;2，即10&#x2F;2&#x3D;5，特征图尺寸变为5x5，特征图张数不变。\n之后进入卷积层C5，120张1x1全连接后的特征图，与S4全连接。\n\n本文图片及内容均参考或来自如下资料：\n[1] Udacity的Deep Learning课程：https://cn.udacity.com/course/deep-learning--ud730\n[2] Krizhevsky A, Sutskever I, Hinton G E. ImageNet Classification with Deep Convolutional Neural Networks[J]. Advances in Neural Information Processing Systems, 2012, 25(2):2012.\n[3] Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.\n[5] http://www.jeyzhang.com/cnn-learning-notes-1.html \n[6] http://blog.csdn.net/stdcoutzyx/article/details/41596663\n[7] CS231n: Convolutional Neural Networks for Visual Recognition\n[8] http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/ \n本文是个人的学习笔记，水平有限，如有疏漏，敬请指出，不胜感谢。 \n","categories":["Deep Learning"],"tags":["Deep Learning"]},{"title":"MATLAB中矢量场图的绘制 (quiver/quiver3/dfield/pplane)","url":"/2016/09/01/matlab-plot-vector-field/","content":"quiver函数一般用于绘制二维矢量场图，函数调用方法如下：\nquiver(x,y,u,v)\n\n该函数展示了点(x,y)对应的的矢量(u,v)。其中，x的长度要求等于u、v的列数，y的长度要求等于u、v的行数。在绘制图像的过程中，通常用meshgrid来生成所需的网格采样点。\n下面举几个例子：\n例1：一个最简单的例子，该二维矢量场图中的矢量皆从(0,0)出发，分别指向(1,0) 、(-1,0) 、(0,1) 、(0,-1)。\nx=[0 0 0 0];y=x;u=[1 -1 0 0];v=[0 0 1 -1];quiver(x,y,u,v)\n画出下图\n但我们发现箭头并没有完全指到(1,0) 、(-1,0) 、(0,1) 、(0,-1) 。如果需要箭头完全指到(1,0) 、(-1,0) 、(0,1) 、(0,-1)，我们需要改变scale参数，将其设为1。参考方法如下：\nquiver(x,y,u,v,1)\n　画出图像如下 ：\n当然，也可以改变颜色。改变颜色可以参考LineSpec的设置，参考代码如下：\n&gt;&gt; quiver(x,y,u,v,&#x27;-r&#x27;)  %这里将图像设置为红色\n\n　画出图像如下：\n例2：(参考MathWorks)：已知$$u&#x3D;y\\ cos(x), v &#x3D; y\\ sin(x)$$\n[x,y] = meshgrid(0:0.2:2,0:0.2:2);  %生成所需的网格采样点 x与y在0到2区间 每隔0.2取一个点u = cos(x).*y;v = sin(x).*y;quiver(x,y,u,v) %绘制二维矢量场图\n\n画出下图:\nquiver3函数用法与quiver类似，用于三维矢量场图的绘制。\n例3: (参考MathWorks)绘制$z&#x3D;y^2-x^2$的三维矢量场图。\n&gt;&gt; [x,y]=meshgrid(-3:.5:3,-3:.5:3); %生成所需的网格采样点 x与y在-3到3范围内 每隔0.5取一个点&gt;&gt; z=y.^2-x.^2;&gt;&gt; [u,v,w]=surfnorm(z); %取三维曲面的法线&gt;&gt; quiver3(z,u,v,w)  %绘制三维矢量场图\n画出下图：\ndfield与pplane(多应用于常微分方程)dfield与pplane的原作者是Rice University的John C. Polking，用于解决涉及常微分方程的问题，比较方便，这里可以下载dfield与pplane的.m文件\n在MATLAB中调用dfield，呈现\n如果我们要绘制常微分方程$x’&#x3D;x^2-t$ 对应的矢量场图，我们可以输入对应的公式与参数值。在这里，上图中默认的常微分方程对应矢量场图：\n在MATLAB中调用pplane，呈现以默认的微分方程为例，可以绘制矢量场图：\n","categories":["Programming Tutorial","MATLAB","Mathematics"],"tags":["Mathematics","Vector Calculus","MATLAB","Scientific Computing","Data Visualization","Programming Tutorial"]}]