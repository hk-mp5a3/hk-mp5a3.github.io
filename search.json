[{"title":"浅谈深度学习中的激活函数 The Activation Function in Deep Learning","url":"/2016/11/11/activation-function-in-dl/","content":"激活函数的作用首先，激活函数不是真的要去激活什么。在神经网络中，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。比如在下面的这个问题中：\n\n如上图(图片来源)，在最简单的情况下，数据是线性可分的，只需要一条直线就已经能够对样本进行很好地分类。但如果情况变得复杂了一点呢？在上图中(图片来源)，数据就变成了线性不可分的情况。在这种情况下，简单的一条直线就已经不能够对样本进行很好地分类了。于是我们尝试引入非线性的因素，对样本进行分类。\n在神经网络中也类似，我们需要引入一些非线性的因素，来更好地解决复杂的问题。而激活函数恰好能够帮助我们引入非线性因素，它使得我们的神经网络能够更好地解决较为复杂的问题。\n\n激活函数的定义及其相关概念在ICML2016的一篇论文Noisy Activation Functions中，作者将激活函数定义为一个几乎处处可微的 h : R → R 。\n在实际应用中，我们还会涉及到以下的一些概念：a.饱和当一个激活函数$h(x)$满足$$\\lim_{n\\to +\\infty} h’(x)&#x3D;0$$时我们称之为右饱和。\n当一个激活函数$h(x)$满足$$\\lim_{n\\to -\\infty} h’(x)&#x3D;0$$时我们称之为左饱和。当一个激活函数，既满足左饱和又满足又饱和时，我们称之为饱和。\nb.硬饱和与软饱和对任意的$x$，如果存在常数$c$，当$x &gt; c$时恒有 $h’(x) &#x3D; 0$则称其为右硬饱和，当$x &lt; c$时恒 有$h’(x)&#x3D;0$则称其为左硬饱和。若既满足左硬饱和，又满足右硬饱和，则称这种激活函数为硬饱和。但如果只有在极限状态下偏导数等于0的函数，称之为软饱和。\n\nSigmoid函数Sigmoid函数曾被广泛地应用，但由于其自身的一些缺陷，现在很少被使用了。Sigmoid函数被定义为：$$f(x)&#x3D;\\frac{1}{1+e^{-x}}$$函数对应的图像是：\n优点：\n\nSigmoid函数的输出映射在$(0,1)$之间，单调连续，输出范围有限，优化稳定，可以用作输出层。\n求导容易。\n\n缺点：\n\n由于其软饱和性，容易产生梯度消失，导致训练出现问题。\n其输出并不是以0为中心的。\n\n\ntanh函数现在，比起Sigmoid函数我们通常更倾向于tanh函数。tanh函数被定义为$$tanh(x)&#x3D;\\frac{1-e^{-2x}}{1+e^{-2x}}$$函数位于[-1, 1]区间上，对应的图像是：优点：\n\n比Sigmoid函数收敛速度更快。\n相比Sigmoid函数，其输出以0为中心。缺点：还是没有改变Sigmoid函数的最大问题——由于饱和性产生的梯度消失。\n\n\nReLUReLU是最近几年非常受欢迎的激活函数。被定义为$$y&#x3D;\\begin{cases}0&amp; (x\\le0)\\\\x&amp; (x&gt;0)\\end{cases}$$对应的图像是：但是除了ReLU本身的之外，TensorFlow还提供了一些相关的函数，比如定义为min(max(features, 0), 6)的tf.nn.relu6(features, name=None)或是CReLU，即tf.nn.crelu(features, name=None)。其中CReLU部分可以参考这篇论文。优点：\n\n相比起Sigmoid和tanh，ReLU(e.g. a factor of 6 in Krizhevsky et al.)在SGD中能够快速收敛。例如在下图的实验中，在一个四层的卷积神经网络中，实线代表了ReLU，虚线代表了tanh，ReLU比起tanh更快地到达了错误率0.25处。据称，这是因为它线性、非饱和的形式。\nSigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现。\n有效缓解了梯度消失的问题。\n在没有无监督预训练的时候也能有较好的表现。\n提供了神经网络的稀疏表达能力。\n\n缺点：随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了。\n\nLReLU、PReLU与RReLU\n通常在LReLU和PReLU中，我们定义一个激活函数为$$f(y_i)&#x3D;\\begin{cases}y_i&amp; if(y_i&gt;0)\\\\a_iy_i&amp; if(y_i\\le0)\\end{cases}$$\n\nLReLU当$a_i$比较小而且固定的时候，我们称之为LReLU。LReLU最初的目的是为了避免梯度消失。但在一些实验中，我们发现LReLU对准确率并没有太大的影响。很多时候，当我们想要应用LReLU时，我们必须要非常小心谨慎地重复训练，选取出合适的$a$，LReLU的表现出的结果才比ReLU好。因此有人提出了一种自适应地从数据中学习参数的PReLU。\n\nPReLUPReLU是LReLU的改进，可以自适应地从数据中学习参数。PReLU具有收敛速度快、错误率低的特点。PReLU可以用于反向传播的训练，可以与其他层同时优化。 在论文Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification中，作者就对比了PReLU和ReLU在ImageNet model A的训练效果。值得一提的是，在tflearn中有现成的LReLU和PReLU可以直接用。\n\nRReLU在RReLU中，我们有$$y_{ji}&#x3D;\\begin{cases}x_{ji}&amp; if(x_{ji}&gt;0)\\\\a_{ji}x_{ji}&amp; if(x_{ji}\\le0)\\end{cases}$$$$a_{ji} \\sim U(l,u),l&lt;u\\ and \\ l,u\\in [0,1) $$其中，$a_{ji}$是一个保持在给定范围内取样的随机变量，在测试中是固定的。RReLU在一定程度上能起到正则效果。在论文Empirical Evaluation of Rectified Activations in Convolution Network中，作者对比了RReLU、LReLU、PReLU、ReLU 在CIFAR-10、CIFAR-100、NDSB网络中的效果。\n\n\n\nELUELU被定义为$$f(x)&#x3D;\\begin{cases}a(e^x-1)&amp; if(x&lt;0)\\\\x&amp; if(0\\le x)\\end{cases}$$其中$a&gt;0$。\n优点：\n\nELU减少了正常梯度与单位自然梯度之间的差距，从而加快了学习。\n在负的限制条件下能够更有鲁棒性。\n\nELU相关部分可以参考这篇论文。\n\nSoftplus与SoftsignSoftplus被定义为$$f(x)&#x3D;log(e^x+1)$$Softsign被定义为$$f(x)&#x3D;\\frac{x}{|x|+1}$$目前使用的比较少，在这里就不详细讨论了。TensorFlow里也有现成的可供使用。激活函数相关TensorFlow的官方文档\n\n总结关于激活函数的选取，目前还不存在定论，实践过程中更多还是需要结合实际情况，考虑不同激活函数的优缺点综合使用。同时，也期待越来越多的新想法，改进目前存在的不足。\n\n文章部分图片或内容参考自：CS231n Convolutional Neural Networks for Visual RecognitionQuora - What is the role of the activation function in a neural network?深度学习中的激活函数导引Noisy Activation Functions－ICML2016\n\n\n本文为作者的个人学习笔记，转载请先声明。如有疏漏，欢迎指出，不胜感谢。\n","categories":["Machine Learning","Deep Learning"],"tags":["Machine Learning","Deep Learning"]},{"title":"MATLAB中矢量场图的绘制 (quiver/quiver3/dfield/pplane)","url":"/2016/09/01/matlab-plot-vector-field/","content":"quiver函数一般用于绘制二维矢量场图，函数调用方法如下：\nquiver(x,y,u,v)\n\n该函数展示了点(x,y)对应的的矢量(u,v)。其中，x的长度要求等于u、v的列数，y的长度要求等于u、v的行数。在绘制图像的过程中，通常用meshgrid来生成所需的网格采样点。\n\n\n下面举几个例子：\n例1：一个最简单的例子，该二维矢量场图中的矢量皆从(0,0)出发，分别指向(1,0) 、(-1,0) 、(0,1) 、(0,-1)。\nx=[0 0 0 0];y=x;u=[1 -1 0 0];v=[0 0 1 -1];quiver(x,y,u,v)\n画出下图\n但我们发现箭头并没有完全指到(1,0) 、(-1,0) 、(0,1) 、(0,-1) 。如果需要箭头完全指到(1,0) 、(-1,0) 、(0,1) 、(0,-1)，我们需要改变scale参数，将其设为1。参考方法如下：\nquiver(x,y,u,v,1)\n　画出图像如下 ：\n当然，也可以改变颜色。改变颜色可以参考LineSpec的设置，参考代码如下：\n&gt;&gt; quiver(x,y,u,v,&#x27;-r&#x27;)  %这里将图像设置为红色\n\n　画出图像如下：\n例2：(参考MathWorks)：已知$$u&#x3D;y\\ cos(x), v &#x3D; y\\ sin(x)$$\n[x,y] = meshgrid(0:0.2:2,0:0.2:2);  %生成所需的网格采样点 x与y在0到2区间 每隔0.2取一个点u = cos(x).*y;v = sin(x).*y;quiver(x,y,u,v) %绘制二维矢量场图\n\n画出下图:\nquiver3函数用法与quiver类似，用于三维矢量场图的绘制。\n例3: (参考MathWorks)绘制$z&#x3D;y^2-x^2$的三维矢量场图。\n&gt;&gt; [x,y]=meshgrid(-3:.5:3,-3:.5:3); %生成所需的网格采样点 x与y在-3到3范围内 每隔0.5取一个点&gt;&gt; z=y.^2-x.^2;&gt;&gt; [u,v,w]=surfnorm(z); %取三维曲面的法线&gt;&gt; quiver3(z,u,v,w)  %绘制三维矢量场图\n画出下图：\ndfield与pplane(多应用于常微分方程)dfield与pplane的原作者是Rice University的John C. Polking，用于解决涉及常微分方程的问题，比较方便，这里可以下载dfield与pplane的.m文件\n在MATLAB中调用dfield，呈现\n如果我们要绘制常微分方程$x’&#x3D;x^2-t$ 对应的矢量场图，我们可以输入对应的公式与参数值。在这里，上图中默认的常微分方程对应矢量场图：\n在MATLAB中调用pplane，呈现以默认的微分方程为例，可以绘制矢量场图：\n","categories":["Programming Tutorial","MATLAB","Mathematics","Vector Calculus","Scientific Computing","Data Visualization"],"tags":["Mathematics","Vector Calculus","MATLAB","Scientific Computing","Data Visualization","Programming Tutorial"]}]