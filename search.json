[{"title":"深度学习笔记 (一)  卷积神经网络基础 (Foundation of Convolutional Neural Networks)","url":"/2016/11/07/foundation-of-cnn-1/","content":"卷积卷积神经网络(Convolutional Neural Networks)是一种在空间上共享参数的神经网络。使用数层卷积，而不是数层的矩阵相乘。在图像的处理过程中，每一张图片都可以看成一张“薄饼”，其中包括了图片的高度、宽度和深度（即颜色，用RGB表示）。\n\n\n在不改变权重的情况下，把这个上方具有k个输出的小神经网络对应的小块滑遍整个图像，可以得到一个宽度、高度不同，而且深度也不同的新图像。\n卷积时有很多种填充图像的方法，以下主要介绍两种，一种是相同填充，一种是有效填充。\n如图中紫色方框所示，左边是有效填充，右边是相同填充。在相同填充中，超出边界的部分使用补充0的办法，使得输入输出的图像尺寸相同。而在有效填充中，则不使用补充0的方法，不能超出边界，因此往往输入的尺寸大于输出的尺寸。\n下图展示了以3x3的网格在28x28的图像上，使用不同步长、填充方法填充所得到的输出图像的尺寸：\n下面借助两个动图来理解一下卷积的过程：\n第一种是以3x3的网格在5x5的图像上进行有效填充的卷积过程：\n第二种是使用3x3的网格在5x5图像上进行相同填充的卷积过程，动图在：http://cs231n.github.io/convolutional-networks/\n回顾整个过程，就是一层一层地增加网络深度，最终得到一个又深又窄的表示，然后把其连接到全连接层，然后训练分类器。\n\n局部连接与权重共享\n总体而言，局部连接和权重共享都是减少参数的办法，使得特征提取更为有效。\n上图中左半部分，是全连接神经网络的示例。图中是一个1000x1000的图像，下一隐藏层有$10^6$个神经元，那么就会有1000x1000x$10^6$&#x3D;$10^{12}$个参数。\n上图右半部分，是局部连接神经网络的示例。图中依然是一个1000x1000的图像，下一隐藏层有$10^6$个神经元，但是使用了一个10x10的卷积核，连接到了10x10的局部图像，那么则会有10x10x$10^6$&#x3D;$10^8$个参数。\n可见局部连接可以很大幅度减少参数的数量。\n\n在实际应用中，有一些情况比较特殊，涉及到了统计不变性的问题。比如我们想识别图像中的动物类别，那么动物在图片中的位置（左上角、中间或是右下角）是不重要的，这叫平移不变性；再比如说，在识别数字的过程中，数字的颜色并不影响结果；又或者说，在语言处理中，一些词汇在句子中的位置并不影响其代表的含义。当两种输入可以获得同样的信息，那么我们就应该共享权重而且利用这些输入来共同训练权重。\n在上图中的左半部分，是未使用权重共享的局部连接神经网络的示例。\n在上图中的右半部分，则使用了权重共享。图中是一个1000x1000的图像，有100个10x10的卷积核，最终会有100x10x10&#x3D;10k个参数。使用局部连接和权重共享都大大地减小了参数数量。而共享权重使得统计不变性问题得到了有效解决。\n\n池化\n通过卷积后，为了引入不变性，同时防止过拟合问题或欠拟合问题、降低计算量，我们常进行池化处理。池化过程如上图所示。因此池化过后，通常图像的宽度和高度会变成原来的1&#x2F;2。\n其中包括了Max pooling 、 Mean pooling和Stochastic pooling三种池化方法。\n两种较为常用的是：Max pooling和Mean pooling。Max pooling是选择kernel范围之内的最大值；Mean pooling则是选择kernel范围之内的平均值。\n\nLeNet-5中的卷积与池化分析\n在LeNet-5中，输入层是32x32的尺寸。\n在第一次卷积中，使用了6个卷积核，得到了C1:6张28x28的特征图。\n然后进行下采样，得到S2:特征图宽、高变为原来的1&#x2F;2，即28&#x2F;2&#x3D;14，特征图尺寸变为14x14，特征图张数不变。\n再进行第二次卷积，使用了16个卷积核，得到了C3:16张10x10的特征图。\n然后进行下采样，得到S4:特征图宽、高变为原来的1&#x2F;2，即10&#x2F;2&#x3D;5，特征图尺寸变为5x5，特征图张数不变。\n之后进入卷积层C5，120张1x1全连接后的特征图，与S4全连接。\n\n本文图片及内容均参考或来自如下资料：\n[1] Udacity的Deep Learning课程：https://cn.udacity.com/course/deep-learning--ud730\n[2] Krizhevsky A, Sutskever I, Hinton G E. ImageNet Classification with Deep Convolutional Neural Networks[J]. Advances in Neural Information Processing Systems, 2012, 25(2):2012.\n[3] Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.\n[5] http://www.jeyzhang.com/cnn-learning-notes-1.html \n[6] http://blog.csdn.net/stdcoutzyx/article/details/41596663\n[7] CS231n: Convolutional Neural Networks for Visual Recognition\n[8] http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/ \n本文是个人的学习笔记，水平有限，如有疏漏，敬请指出，不胜感谢。 \n","categories":["Deep Learning"],"tags":["Deep Learning","TensorFlow"]},{"title":"深度学习笔记 (二) 在TensorFlow上训练一个多层卷积神经网络","url":"/2016/11/10/foundation-of-cnn-2/","content":"上一篇笔记主要介绍了卷积神经网络相关的基础知识。在本篇笔记中，将参考TensorFlow官方文档使用mnist数据集，在TensorFlow上训练一个多层卷积神经网络。\n\n\n下载并导入mnist数据集 首先，利用input_data.py来下载并导入mnist数据集。在这个过程中，数据集会被下载并存储到名为”MNIST_data”的目录中。\nimport input_datamnist = input_data.read_data_sets(&#x27;MNIST_data&#x27;, one_hot=True)\n其中mnist是一个轻量级的类，其中以Numpy数组的形式中存储着训练集、验证集、测试集。\n\n进入一个交互式的TensorFlow会话TensorFlow实际上对应的是一个C++后端，TensorFlow使用会话(Session)与后端连接。通常，我们都会先创建一个图，然后再在会话(Session)中启动它。而InteractiveSession给了我们一个交互式会话的机会，使得我们可以在运行图(Graph)的时候再插入计算图，否则就要在启动会话之前构建整个计算图。使用InteractiveSession会使得我们的工作更加便利，所以大部分情况下，尤其是在交互环境下，我们都会选择InteractiveSession。\nimport tensorflow as tfsess = tf.InteractiveSession()\n\n\n利用占位符处理输入数据 关于占位符的概念，官方给出的解释是“不是特定的值，而是可以在TensorFlow运行某一计算时根据该占位符输入具体的值”。这里也比较容易理解。\nx = tf.placeholder(&quot;float&quot;, shape=[None, 784])\nx代表的是输入图片的浮点数张量，因此定义dtype为float。其中，shape的None代表了没有指定张量的shape，可以feed任何shape的张量，在这里指batch的大小未定。一张mnist图像的大小是28*28，784是一张展平的mnist图像的维度，即28*28＝784。\ny_ = tf.placeholder(&quot;float&quot;, shape=[None, 10])\n由于mnist数据集是手写数字的数据集，所以分的类别也只有10类，分别代表了0～9十个数字。\n\n权重与偏置项初始化在对权重初始化的过程中，我们加入少量的噪声来打破对称性与避免梯度消失，在这里我们设定权重的标准差为0.1。 由于我们使用的激活函数是ReLU，而ReLU的定义是$$y&#x3D;\\begin{cases}0&amp; (x\\ge0)\\\\x&amp; (x&lt;0)\\end{cases}$$ReLU对应的图像是下图左边的函数图像。但是我们可以注意到，ReLU在$x&lt;0$的部分是硬饱和的，所以随着训练推进，部分输入可能会落到硬饱和区，导致权重无法更新，出现“神经元死亡”。虽然在之后的研究中，有人提出了PReLU和ELU等新的激活函数来改进，但我们在这里的训练，还是应该用一个较小的正数来初始化偏置项，避免神经元节点输出恒为0的问题。\n#初始化权重def weight_variable(shape):    initial = tf.truncated_normal(shape, stddev=0.1)    return tf.Variable(initial)# 初始化偏置项def bias_variable(shape):    initial = tf.constant(0.1, shape=shape)    return tf.Variable(initial)\n\n\n卷积与池化在这里，我们使用步长为1、相同填充(padding&#x3D;’SAME’)的办法进行卷积，关于相同填充和有效填充的区别在上一篇笔记讲得比较清楚了，在这里就不赘述了。与此同时，使用2x2的网格以max pooling的方法池化。\n# 卷积过程def conv2d(x, w):    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;)# 池化过程def max_pool_2x2(x):    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&quot;SAME&quot;)\n\n\n第一层卷积(1 #28x28-&gt;32 #28x28)首先在每个5x5网格中，提取出32张特征图。其中weight_variable中前两维是指网格的大小，第三维的1是指输入通道数目，第四维的32是指输出通道数目（也可以理解为使用的卷积核个数、得到的特征图张数）。每个输出通道都有一个偏置项，因此偏置项个数为32。\nw_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32])\n为了使之能用于计算，我们使用reshape将其转换为四维的tensor，其中第一维的－1是指我们可以先不指定，第二三维是指图像的大小，第四维对应颜色通道数目，灰度图对应1，rgb图对应3.\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n而后，我们利用ReLU激活函数，对其进行第一次卷积。\nh_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n\n\n第一次池化(32 #28x28-&gt;32 #14x14)比较容易理解，使用2x2的网格以max pooling的方法池化。\nh_pool1 = max_pool_2x2(h_conv1)\n\n\n第二层卷积与第二次池化(32 #14x14-&gt;64 #14x14-&gt;64 #7x7)与第一层卷积、第一次池化类似的过程。\nw_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)\n\n\n密集连接层此时，图片是7x7的大小。我们在这里加入一个有1024个神经元的全连接层。之后把刚才池化后输出的张量reshape成一个一维向量，再将其与权重相乘，加上偏置项，再通过一个ReLU激活函数。\nw_fc1=weight_variable([7*7*64,1024])b_fc1=bias_variable([1024])h_pool2_flat=tf.reshape(h_pool2,[-1,7*7*64])h_fc1=tf.nn.relu(tf.matmul(h_pool2_flat,w_fc1)+b_fc1)\n\n\nDropoutkeep_prob=tf.placeholder(&quot;float&quot;)h_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)\n这是一个比较新的也非常好用的防止过拟合的方法，想出这个方法的人基本属于非常crazy的存在。在Udacity-Deep Learning的课程中有提到这个方法——完全随机选取经过神经网络流一半的数据来训练，在每次训练过程中用0来替代被丢掉的激活值，其它激活值合理缩放。\n\n类别预测与输出应用了简单的softmax，输出。\nw_fc2=weight_variable([1024,10])b_fc2=bias_variable([10])y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop,w_fc2)+b_fc2)\n\n\n模型的评价#计算交叉熵的代价函数cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))#使用优化算法使得代价函数最小化train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)#找出预测正确的标签correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))#得出通过正确个数除以总数得出准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))sess.run(tf.initialize_all_variables())#每100次迭代输出一次日志，共迭代20000次for i in range(20000):  batch = mnist.train.next_batch(50)  if i%100 == 0:    train_accuracy = accuracy.eval(feed_dict=&#123;        x:batch[0], y_: batch[1], keep_prob: 1.0&#125;)    print &quot;step %d, training accuracy %g&quot;%(i, train_accuracy)  train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)print &quot;test accuracy %g&quot;%accuracy.eval(feed_dict=&#123;    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;)\n\n\n本文代码与部分内容来源或参考自：TensorFlow官方文档深度学习中的激活函数导引Udacity-Deep Learning\n","categories":["Deep Learning"],"tags":["Deep Learning","TensorFlow"]},{"title":"机器学习|如何处理机器学习中的非均衡数据集？","url":"/2018/08/22/handle-imbalanced-data/","content":"在机器学习中，我们常常会遇到不均衡的数据集。比如癌症数据集中，癌症样本的数量可能远少于非癌症样本的数量；在银行的信用数据集中，按期还款的客户数量可能远大于违约客户的样本数量。 比如非常有名的德国信用数据集，正负样本的分类就不是很均衡：\n\n\n\n\n好客户(Good, 0)\n坏客户(Bad, 1)\n\n\n\n样本数量\n700\n300\n\n\n如果不做任何处理简单地进行训练，那么训练结果中（以SVM为例），大部分好客户（约97%）能被正确地识别为好客户，但是大部分的坏客户（约95%）却会被识别为好客户。这个时候，如果我们仅仅使用accuracy来评价模型，那么银行可能会承受违约带来的巨大损失。在南大周志华老师的《机器学习》“模型的选择与评价”部分中，就提到了使用Precision、Recall、F1 Score(加权平均Precision和Recall)等更全面评价模型的方法。本文将探讨如何解决机器学习中遇到的分类非均衡问题：\n\n过采样 Over-sampling\n下采样 Under-sampling\n上采样与下采样结合\n集成采样 Ensemble sampling\n代价敏感学习 Cost-Sensitive Learning\n\n注：github开源项目github-scikit-learn-contrib&#x2F;imbalanced-learn中提供了本回答中大部分算法的实现代码，并配有详细的文档和注释。\n\n过采样 Over-sampling过采样即是将本来数量少的那类样本增加。目前比较常见的方法包含了SMOTE, ADASYN, SVM SMOTE,bSMOTE。其中，SMOTE和ADASYN算法的实现也可以参考这个github项目。\n我可视化了一下结果：比如说在下图中，蓝色三角形代表的是多数样本(不妨设为正例)，绿色三角形代表的是原始的少数样本(不妨设为反例)，而红色圆点则是使用SMOTE算法生成的反例。类似地，ADASYN也可以有类似的效果。不过，SMOTE在一些情况下表现得并不是特别好，也不是很稳定，这也与它本身的算法思路有关。我们可以对比一下在下面情况下SMOTE和ADASYN的表现：但ADASYN也不是完美无缺的——当分割两个类别样本能够清晰地被划分而且数据点间隔很大时ADASYN会出现NaN。例如在以下的情况，ADASYN就很可能会出问题：\n\nSMOTE：SMOTE: Synthetic Minority Over-sampling Technique\nADASYN：ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning\n\n\n下采样 Under-sampling下采样即是将本来数量多的那类样本减少。随机下采样就不用说了，实现非常简单。但它的表现并不是很好，因此又有了一些新方法，比较知名的有：\n\nTomek links\nOne-sided selection: Addressing the curse of imbalanced training sets: One-sided selection\nNeighboorhood Cleaning Rule: Improving identification of difficult small classes by balancing class distribution\nNearMiss: kNN approach to unbalanced data distributions: A case study involving information extraction\n\n\n上采样与下采样结合顾名思义，将原本比较多的样本所属类别的样本减少，同时也将原本属于少数的样本类别中的样本增加。\n\n集成采样 Ensemble sampling如我们所知，一些下采样的方法可能会使我们丢失一些比较重要的数据点，但是Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou的论文Exploratory Undersampling for Class-Imbalance Learning中提出了EasyEnsemble和BalanceCascade的方法一定程度上解决了这个问题。在论文中，作者提到，EasyEnsemble的思想有一部分与Balanced Random Forests相似，但是EasyEnsemble使用了样本来随机训练决策树。\n\n代价敏感学习 Cost-Sensitive Learning我们都知道比起将一个正常客户误判为不良贷款客户，将一个不良贷款客户误判为正常客户可能会给银行带来更大的损失；比起将非癌症病人误判为癌症病人，将癌症病人误判为非癌症病人可能会导致治疗无法及时进行从而导致更严重的后果。于是就有了cost-sensitive learning这样的思路——来解决这种样本分类不均衡的问题。这部分可以参考Charles X. Ling, Victor S. Sheng的论文Cost-Sensitive Learning and the Class Imbalance Problem：\n \n","categories":["Machine Learning"],"tags":["Machine Learning"]},{"title":"MATLAB中矢量场图的绘制 (quiver/quiver3/dfield/pplane)","url":"/2016/09/01/matlab-plot-vector-field/","content":"quiver函数一般用于绘制二维矢量场图，函数调用方法如下：\nquiver(x,y,u,v)\n\n该函数展示了点(x,y)对应的的矢量(u,v)。其中，x的长度要求等于u、v的列数，y的长度要求等于u、v的行数。在绘制图像的过程中，通常用meshgrid来生成所需的网格采样点。\n下面举几个例子：\n例1：一个最简单的例子，该二维矢量场图中的矢量皆从(0,0)出发，分别指向(1,0) 、(-1,0) 、(0,1) 、(0,-1)。\nx=[0 0 0 0];y=x;u=[1 -1 0 0];v=[0 0 1 -1];quiver(x,y,u,v)\n画出下图\n但我们发现箭头并没有完全指到(1,0) 、(-1,0) 、(0,1) 、(0,-1) 。如果需要箭头完全指到(1,0) 、(-1,0) 、(0,1) 、(0,-1)，我们需要改变scale参数，将其设为1。参考方法如下：\nquiver(x,y,u,v,1)\n　画出图像如下 ：\n当然，也可以改变颜色。改变颜色可以参考LineSpec的设置，参考代码如下：\n&gt;&gt; quiver(x,y,u,v,&#x27;-r&#x27;)  %这里将图像设置为红色\n\n　画出图像如下：\n例2：(参考MathWorks)：已知$$u&#x3D;y\\ cos(x), v &#x3D; y\\ sin(x)$$\n[x,y] = meshgrid(0:0.2:2,0:0.2:2);  %生成所需的网格采样点 x与y在0到2区间 每隔0.2取一个点u = cos(x).*y;v = sin(x).*y;quiver(x,y,u,v) %绘制二维矢量场图\n\n画出下图:\nquiver3函数用法与quiver类似，用于三维矢量场图的绘制。\n例3: (参考MathWorks)绘制$z&#x3D;y^2-x^2$的三维矢量场图。\n&gt;&gt; [x,y]=meshgrid(-3:.5:3,-3:.5:3); %生成所需的网格采样点 x与y在-3到3范围内 每隔0.5取一个点&gt;&gt; z=y.^2-x.^2;&gt;&gt; [u,v,w]=surfnorm(z); %取三维曲面的法线&gt;&gt; quiver3(z,u,v,w)  %绘制三维矢量场图\n画出下图：\ndfield与pplane(多应用于常微分方程)dfield与pplane的原作者是Rice University的John C. Polking，用于解决涉及常微分方程的问题，比较方便，这里可以下载dfield与pplane的.m文件\n在MATLAB中调用dfield，呈现\n如果我们要绘制常微分方程$x’&#x3D;x^2-t$ 对应的矢量场图，我们可以输入对应的公式与参数值。在这里，上图中默认的常微分方程对应矢量场图：\n在MATLAB中调用pplane，呈现以默认的微分方程为例，可以绘制矢量场图：\n","categories":["Programming Tutorial","MATLAB","Mathematics"],"tags":["Mathematics","Vector Calculus","MATLAB","Scientific Computing","Data Visualization","Programming Tutorial"]},{"title":"TensorFlow | ReluGrad input is not finite. Tensor had NaN values","url":"/2017/07/24/relugrad-input-is-not-finite/","content":"问题的出现这个问题是我基于TensorFlow使用CNN训练MNIST数据集的时候遇到的。关键的相关代码是以下这部分：\n\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n学习速率是$(1e-4)$的时候是没有问题，但是当我把学习速率调到$0.01&#x2F;0.5$的时候，很快就会报错。\ntensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values\n\n分析学习速率于是我尝试加上几行代码，希望能把y_conv和cross_entropy的状态反映出来。\ny_conv = tf.Print(y_conv, [y_conv], &quot;y_conv: &quot;)cross_entropy = tf.Print(cross_entropy, [cross_entropy], &quot;cross_entropy: &quot;)\n当learning rate&#x3D;0.01时，程序会报错：\nI tensorflow/core/kernels/logging_ops.cc:64] y_conv: [3.0374929e-06 0.0059775524 0.980205...]step 0, training accuracy 0.04I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [9.2028862e-10 1.4812358e-05 0.044873074...]I tensorflow/core/kernels/logging_ops.cc:64] cross_entropy: [648.49146]I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [0.024463326 1.4828938e-31 0...]step 1, training accuracy 0.2I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [2.4634053e-11 3.3087209e-34 0...]I tensorflow/core/kernels/logging_ops.cc:64] cross_entropy: [nan]step 2, training accuracy 0.14I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [nan nan nan...]W tensorflow/core/common_runtime/executor.cc:1027] 0x7ff51d92a940 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n当learning rate $&#x3D;1e-4$时，程序不会报错。\nI tensorflow/core/kernels/logging_ops.cc:64] y_conv: [0.00056920078 8.4922984e-09 0.00033719366...]step 0, training accuracy 0.14I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [7.0613837e-10 9.28294e-09 0.00016230672...]I tensorflow/core/kernels/logging_ops.cc:64] cross_entropy: [439.95135]step 1, training accuracy 0.16I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [0.031509314 3.6221365e-05 0.015359053...]I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [3.7112056e-07 1.8543299e-09 8.9234991e-06...]I tensorflow/core/kernels/logging_ops.cc:64] cross_entropy: [436.37653]step 2, training accuracy 0.12I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [0.015578311 0.0026688741 0.44736364...]I tensorflow/core/kernels/logging_ops.cc:64] y_conv: [6.0428465e-07 0.0001744287 0.026451336...]I tensorflow/core/kernels/logging_ops.cc:64] cross_entropy: [385.33765]\n至此，我们可以看到，学习速率太大是产生error其中一个原因。\n参考斯坦福CS 224D的Lecture Note，在训练深度神经网络的时候，出现NaN比较大的可能是因为学习速率过大，梯度值过大，产生梯度爆炸。\n\nDuring experimentation, once the gradient value grows extremely large, it causes an overflow (i.e. NaN) which is easily detectable at runtime; this issue is called the Gradient Explosion Problem.\n\n解决方法\n适当减小学习速率 \n加入Gradient clipping的方法。 Gradient clipping的方法最早是由Thomas Mikolov提出的。每当梯度达到一定的阈值，就把他们设置回一个小一些的数字。Refer to the lecture note of Stanford CS 224D, use gradient clipping. \nTo solve the problem of exploding gradients, Thomas Mikolov first introduced a simple heuristic solution that clips gradients to a small number whenever they explode. That is, whenever they reach a certain threshold, they are set back to a small number as shown in Algorithm 1.\n\n\n\nAlgorithm 1:$\\frac{\\partial E}{\\partial W}\\to g$if $ \\Vert g\\Vert\\ge threshold$  then$\\frac {threshold}{\\Vert g\\Vert} g\\to g$end if\n","categories":["Deep Learning"],"tags":["Deep Learning","TensorFlow"]},{"title":"浅谈深度学习中的激活函数 The Activation Function in Deep Learning","url":"/2016/11/11/activation-function-in-dl/","content":"激活函数的作用首先，激活函数不是真的要去激活什么。在神经网络中，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。\n\n比如在下面的这个问题中：\n如上图(图片来源)，在最简单的情况下，数据是线性可分的，只需要一条直线就已经能够对样本进行很好地分类。但如果情况变得复杂了一点呢？在上图中(图片来源)，数据就变成了线性不可分的情况。在这种情况下，简单的一条直线就已经不能够对样本进行很好地分类了。于是我们尝试引入非线性的因素，对样本进行分类。\n在神经网络中也类似，我们需要引入一些非线性的因素，来更好地解决复杂的问题。而激活函数恰好能够帮助我们引入非线性因素，它使得我们的神经网络能够更好地解决较为复杂的问题。\n\n激活函数的定义及其相关概念在ICML2016的一篇论文Noisy Activation Functions中，作者将激活函数定义为一个几乎处处可微的 h : R → R 。\n在实际应用中，我们还会涉及到以下的一些概念：a.饱和当一个激活函数$h(x)$满足$$\\lim_{n\\to +\\infty} h’(x)&#x3D;0$$时我们称之为右饱和。\n当一个激活函数$h(x)$满足$$\\lim_{n\\to -\\infty} h’(x)&#x3D;0$$时我们称之为左饱和。当一个激活函数，既满足左饱和又满足又饱和时，我们称之为饱和。\nb.硬饱和与软饱和对任意的$x$，如果存在常数$c$，当$x &gt; c$时恒有 $h’(x) &#x3D; 0$则称其为右硬饱和，当$x &lt; c$时恒 有$h’(x)&#x3D;0$则称其为左硬饱和。若既满足左硬饱和，又满足右硬饱和，则称这种激活函数为硬饱和。但如果只有在极限状态下偏导数等于0的函数，称之为软饱和。\n\nSigmoid函数Sigmoid函数曾被广泛地应用，但由于其自身的一些缺陷，现在很少被使用了。Sigmoid函数被定义为：$$f(x)&#x3D;\\frac{1}{1+e^{-x}}$$函数对应的图像是：\n优点：\n\nSigmoid函数的输出映射在$(0,1)$之间，单调连续，输出范围有限，优化稳定，可以用作输出层。\n求导容易。\n\n缺点：\n\n由于其软饱和性，容易产生梯度消失，导致训练出现问题。\n其输出并不是以0为中心的。\n\n\ntanh函数现在，比起Sigmoid函数我们通常更倾向于tanh函数。tanh函数被定义为$$tanh(x)&#x3D;\\frac{1-e^{-2x}}{1+e^{-2x}}$$函数位于[-1, 1]区间上，对应的图像是：优点：\n\n比Sigmoid函数收敛速度更快。\n相比Sigmoid函数，其输出以0为中心。缺点：还是没有改变Sigmoid函数的最大问题——由于饱和性产生的梯度消失。\n\n\nReLUReLU是最近几年非常受欢迎的激活函数。被定义为$$y&#x3D;\\begin{cases}0&amp; (x\\le0)\\\\x&amp; (x&gt;0)\\end{cases}$$对应的图像是：但是除了ReLU本身的之外，TensorFlow还提供了一些相关的函数，比如定义为min(max(features, 0), 6)的tf.nn.relu6(features, name=None)或是CReLU，即tf.nn.crelu(features, name=None)。其中CReLU部分可以参考这篇论文。优点：\n\n相比起Sigmoid和tanh，ReLU(e.g. a factor of 6 in Krizhevsky et al.)在SGD中能够快速收敛。例如在下图的实验中，在一个四层的卷积神经网络中，实线代表了ReLU，虚线代表了tanh，ReLU比起tanh更快地到达了错误率0.25处。据称，这是因为它线性、非饱和的形式。\nSigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现。\n有效缓解了梯度消失的问题。\n在没有无监督预训练的时候也能有较好的表现。\n提供了神经网络的稀疏表达能力。\n\n缺点：随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了。\n\nLReLU、PReLU与RReLU\n通常在LReLU和PReLU中，我们定义一个激活函数为$$f(y_i)&#x3D;\\begin{cases}y_i&amp; if(y_i&gt;0)\\\\a_iy_i&amp; if(y_i\\le0)\\end{cases}$$\n\nLReLU当$a_i$比较小而且固定的时候，我们称之为LReLU。LReLU最初的目的是为了避免梯度消失。但在一些实验中，我们发现LReLU对准确率并没有太大的影响。很多时候，当我们想要应用LReLU时，我们必须要非常小心谨慎地重复训练，选取出合适的$a$，LReLU的表现出的结果才比ReLU好。因此有人提出了一种自适应地从数据中学习参数的PReLU。\n\nPReLUPReLU是LReLU的改进，可以自适应地从数据中学习参数。PReLU具有收敛速度快、错误率低的特点。PReLU可以用于反向传播的训练，可以与其他层同时优化。 在论文Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification中，作者就对比了PReLU和ReLU在ImageNet model A的训练效果。值得一提的是，在tflearn中有现成的LReLU和PReLU可以直接用。\n\nRReLU在RReLU中，我们有$$y_{ji}&#x3D;\\begin{cases}x_{ji}&amp; if(x_{ji}&gt;0)\\\\a_{ji}x_{ji}&amp; if(x_{ji}\\le0)\\end{cases}$$$$a_{ji} \\sim U(l,u),l&lt;u\\ and \\ l,u\\in [0,1) $$其中，$a_{ji}$是一个保持在给定范围内取样的随机变量，在测试中是固定的。RReLU在一定程度上能起到正则效果。在论文Empirical Evaluation of Rectified Activations in Convolution Network中，作者对比了RReLU、LReLU、PReLU、ReLU 在CIFAR-10、CIFAR-100、NDSB网络中的效果。\n\n\n\nELUELU被定义为$$f(x)&#x3D;\\begin{cases}a(e^x-1)&amp; if(x&lt;0)\\\\x&amp; if(0\\le x)\\end{cases}$$其中$a&gt;0$。\n优点：\n\nELU减少了正常梯度与单位自然梯度之间的差距，从而加快了学习。\n在负的限制条件下能够更有鲁棒性。\n\nELU相关部分可以参考这篇论文。\n\nSoftplus与SoftsignSoftplus被定义为$$f(x)&#x3D;log(e^x+1)$$Softsign被定义为$$f(x)&#x3D;\\frac{x}{|x|+1}$$目前使用的比较少，在这里就不详细讨论了。TensorFlow里也有现成的可供使用。激活函数相关TensorFlow的官方文档\n\n总结关于激活函数的选取，目前还不存在定论，实践过程中更多还是需要结合实际情况，考虑不同激活函数的优缺点综合使用。同时，也期待越来越多的新想法，改进目前存在的不足。\n\n文章部分图片或内容参考自：CS231n Convolutional Neural Networks for Visual RecognitionQuora - What is the role of the activation function in a neural network?深度学习中的激活函数导引Noisy Activation Functions－ICML2016\n\n\n本文为作者的个人学习笔记，转载请先声明。如有疏漏，欢迎指出，不胜感谢。\n","categories":["Deep Learning"],"tags":["Deep Learning"]}]